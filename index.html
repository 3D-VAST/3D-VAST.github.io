<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>3D-VAST: ICCV 2025 Workshop</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@3.3.7/dist/css/bootstrap.min.css">
    <style>
        /* 全局样式 */
        body {
            font-family: 'Segoe UI', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            padding-top: 80px;
        }

        /* 导航栏防遮挡 */
        section::before {
            display: block;
            content: " ";
            margin-top: -80px;
            height: 80px;
            visibility: hidden;
        }

        /* 专业表格样式 */
        .workshop-table {
            border-collapse: collapse;
            width: 100%;
            margin: 25px 0;
            box-shadow: 0 1px 3px rgba(0,0,0,0.1);
        }

        .workshop-table th {
            background: #2c3e50;
            color: white;
            padding: 10px;
            text-align: center;
        }

        .workshop-table td {
            padding: 10px;
            border-bottom: 1px solid #ddd;
            text-align: center;
        }

        /* 人员卡片 */
        .profile-card {
            margin: 20px 0;
            padding: 15px;
            border-radius: 8px;
            background: #f8f9fa;
        }

        .profile-img {
            width: 120px;
            height: 120px;
            border-radius: 50%;
            object-fit: cover;
            margin-right: 20px;
        }

        /* 章节标题 */
        .section-title {
            color: #2c3e50;
            border-left: 4px solid #3498db;
            padding-left: 15px;
            margin: 40px 0 30px;
        }

        /* 添加关键信息高亮 */
        .highlight {
            color: #e74c3c;
            font-weight: bold;
        }

        /* 列表样式 */
        ol.workshop-list {
            padding-left: 20px;
        }

        ol.workshop-list li {
            margin-bottom: 10px;
        }

        /* 时间块样式 */
        .time-block {
            border: 1px solid #e0e0e0;
            border-radius: 8px;
            padding: 15px;
            background: white;
            margin-bottom: 15px;
        }

        .time {
            color: #3498db;
            font-weight: 600;
            margin-bottom: 8px;
            font-size: 0.9em;
        }

        .event h4 {
            color: #2c3e50;
            margin: 0 0 5px 0;
            font-size: 1.1em;
        }

        /* 茶歇特殊样式 */
        .coffee-break {
            background: #f8f9fa;
            border-color: #3498db;
        }
    </style>
</head>
<body>

<!-- 导航栏 -->
<nav class="navbar navbar-default navbar-fixed-top">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#mainNav">
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="#">3D-VAST 2025</a>
        </div>
        <div class="collapse navbar-collapse" id="mainNav">
            <ul class="nav navbar-nav">
                <li><a href="#summary">Summary</a></li>
                <li><a href="#topic">Topic</a></li>
                <li><a href="#organizers">Organizers</a></li>
                <li><a href="#format">Format</a></li>
                <li><a href="#impacts">Impacts</a></li>
                <li><a href="#related">Related Workshops</a></li>
            </ul>
        </div>
    </div>
</nav>

<div class="container">
    <!-- 添加标题 -->
    <h1 style="text-align: center; font-size: 2.5rem; margin: 40px 0; color: #2c3e50;">
        ICCV 2025 Workshop Proposal<br>
        3D-VAST: From street to space: 3D Vision Across Altitudes
    </h1>

  <!-- Summary Section -->
<section id="summary">
    <h2 class="section-title">1. Summary</h2>
    <table class="workshop-table">
        <tr>
            <th>Workshop Title</th>
            <td>From street to space: <span class="highlight">3D V</span>ision <span class="highlight">A</span>cross<span class="highlight">S</span> aI<span class="highlight">T</span>itudes</td>
        </tr>
        <tr>
            <th>Acronym</th>
            <td>3D-VAST</td>
        </tr>
        <tr>
            <th>Edition</th>
            <td>1st</td>
        </tr>
        <tr>
            <th>Half or full day</th>
            <td>Half Day</td>
        </tr>
        <tr>
            <th>Keywords</th>
            <td>Cross-altitude data fusion, aerial images, 3D scene modeling</td>
        </tr>
        <tr>
            <th>Primary Contact</th>
            <td>Yujiao Shi &lt;<a href="mailto:shiyj2@shanghaitech.edu.cn">shiyj2@shanghaitech.edu.cn</a>&gt;</td>
        </tr>
        <tr>
            <th>Anticipated Audience</th>
            <td>100 - 300 participants</td>
        </tr>
        <tr>
            <th>Poster Boards Available</th>
            <td>Around 15</td>
        </tr>
        <tr>
            <th>Proceedings</th>
            <td>Papers will be published</td>
        </tr>
        <!-- 添加论文提交相关的时间节点 -->
        <tr>
            <th style="width: 30%">Paper Submission Deadline</th>
            <td>July 5 '25 (One week after the ICCV notification time)</td>
        </tr>
        <tr>
            <th>Camera Ready Deadline</th>
            <td>October 21 - 23 '25 (Consistent with the ICCV conference time)</td>
        </tr>
    </table>
</section>

	
	<div class="section">
   <h2 class="section-title">2. Topic</h2>
    
    <div class="boxed">
        <p>As large-scale 3D scene modeling becomes increasingly important for applications such as urban planning, robotics, autonomous navigation, and virtual simulations, the need for diverse, high-quality visual data is greater than ever. However, acquiring dense and high-resolution ground-level imagery at scale is often impractical due to access limitations, cost, and environmental variability. In contrast, aerial and satellite imagery provide broader spatial coverage but lack the fine-grained details needed for many downstream applications. Combining images from multiple altitudes — from ground cameras to aerial drones and satellites—offers a promising solution to overcome these limitations, enabling richer, more complete 3D reconstructions.</p>
    </div>
    
    <div class="divider"></div>
    
    <p>How can we achieve coherent and accurate 3D scene modeling <span class="highlight">when our visual world is captured from vastly different altitudes—ground, aerial, and satellite—under varying conditions?</span> Each altitude offers distinct advantages, but cross-altitude data fusion introduces significant challenges: sparse and incomplete views, visual ambiguities, spatio-temporal inconsistencies, image quality variations, dynamic scene changes, and environmental factors that alter topology over time. Traditional 3D reconstruction methods, optimized for dense and structured inputs, struggle with such heterogeneous multi-altitude data. Advances in multi-scale feature alignment, neural scene representations, and robust cross-view fusion offer promising solutions, but key challenges remain.</p>
    
    <div class="divider"></div>
    
    <p>3D-VAST invites researchers and practitioners to explore novel techniques in scene modeling, understanding, rendering, and synthesis across altitudes, bridging the gap between ground-level perspectives and large-scale aerial/satellite observations.</p>
    
    <div class="divider"></div>
    
    <p>The workshop will cover, but not be limited to, the following topics:</p>
    
    <ol>
        <li>Cross-altitude feature matching and registration</li>
        <li>View synthesis from sparse and heterogeneous data sources</li>
        <li>Sparse-view 3D reconstruction (with known or unknown camera poses)</li>
        <li>Generative approaches for view completion and prediction</li>
        <li>Datasets and benchmarks for evaluating cross-altitude vision systems</li>
        <li>Real-world applications in urban planning, simulation, and digital twins</li>
    </ol>
    
    <div class="divider"></div>
    
    <p><strong>Relevance to the Computer Vision community.</strong> This workshop aims to tackle fundamental challenges in 3D scene modeling, data fusion, and multi-view geometry across varying altitudes. By focusing on integrating ground-level, aerial, and satellite imagery, the workshop aims to advance cross-altitude feature matching, view synthesis, and neural scene representations, addressing key issues such as spatial inconsistencies, image quality variations, and dynamic scene changes. These topics are critical for the development of robust computer vision algorithms that can handle heterogeneous, multi-source data to achieve accurate, real-time 3D reconstructions.</p>
    
    <div class="divider"></div>
    
    <p><strong>Potential ICCV 2025 attendees.</strong> The potential attendees of this workshop at ICCV 2025 will include <strong>(i)</strong> computer vision researchers working on 3D reconstruction, multi-view geometry, and data fusion, especially those interested in advancing multi-altitude scene modeling, <strong>(ii)</strong> experts in robotics, autonomous navigation, and urban planning will find the workshop highly relevant, as it addresses the integration of ground, aerial, and satellite data for real-world applications like smart cities, digital twins, and virtual simulations. Additionally, <strong>(iii)</strong> machine learning practitioners exploring neural scene representations, cross-view feature alignment, and generative approaches for 3D reconstruction will benefit from the workshop’s focus on novel computational techniques.</p>
</div>
	
<section id="organizers">
    <h2 class="section-title">3. Organizers & Speakers</h2>
    <h3>Organizing Committee</h3>
    
    <!-- 第一行 -->
    <div class="row mb-4">
        <!-- Yujiao Shi -->
        <div class="col-md-4">
            <div class="profile-card">
                <div class="media" style="display: flex; align-items: center;">       
                    <div class="media-left">
                        <img src="pic/Organizers/Yujiao Shi.jpg" class="profile-img">
                    </div>
                    <div class="media-body">
                        <h4><a href="https://yujiaoshi.github.io/" target="_blank">Yujiao Shi</a></h4>
                        <p>Assistant Professor, ShanghaiTech University</p>
                        <p>Research: Camera localization, 3D reconstruction, view synthesis</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- Yuanbo Xiangli -->
        <div class="col-md-4">
            <div class="profile-card">
                <div class="media" style="display: flex; align-items: center;">
                    <div class="media-left">
                        <img src="pic/Organizers/ambie.jpg" class="profile-img">
                    </div>
                    <div class="media-body">
                        <h4><a href="https://kam1107.github.io/" target="_blank">Yuanbo Xiangli</a></h4>
                        <p>Postdoctoral Researcher, Cornell University</p>
                        <p>Research: 3D city scene reconstruction with multi-source data</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- Zuzana Kukelova -->
        <div class="col-md-4">
            <div class="profile-card">
                <div class="media" style="display: flex; align-items: center;">
                    <div class="media-left">
                        <img src="pic/Organizers/Zuzana Kukelova.jpg" class="profile-img">
                    </div>
                    <div class="media-body">
                        <h4><a href="https://cmp.felk.cvut.cz/~kukelova/" target="_blank">Zuzana Kukelova</a></h4>
                        <p>Assistant Professor, Czech Technical University</p>
                        <p>Research: Camera geometry estimation, minimal problems in CV</p>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- 第二行 -->
    <div class="row">
        <!-- Bo Dai -->
        <div class="col-md-4">
            <div class="profile-card">
                <div class="media" style="display: flex; align-items: center;">
                    <div class="media-left">
                        <img src="pic/Organizers/Bo Dai.jpg" class="profile-img">
                    </div>
                    <div class="media-body">
                        <h4><a href="https://datascience.hku.hk/people/bo-dai/" target="_blank">Bo Dai</a></h4>
                        <p>Assistant Professor, The University of Hong Kong</p>
                        <p>Research: Generative AI for Embodied AI and Metaverse</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- Richard Hartley -->
        <div class="col-md-4">
            <div class="profile-card">
                <div class="media" style="display: flex; align-items: center;">
                    <div class="media-left">
                        <img src="pic/Organizers/Richard-Hartley.png" class="profile-img">
                    </div>
                    <div class="media-body">
                        <h4><a href="https://comp.anu.edu.au/people/richard-hartley/" target="_blank">Richard Hartley</a></h4>
                        <p>Distinguished Professor Emeritus, ANU</p>
                        <p>Research: Multi-view geometry, scene reconstruction</p>
                    </div>
                </div>
            </div>
        </div>

        <!-- Hongdong Li -->
        <div class="col-md-4">
            <div class="profile-card">
                <div class="media" style="display: flex; align-items: center;">
                    <div class="media-left">
                        <img src="pic/Organizers/Hongdong LI.jpg" class="profile-img">
                    </div>
                    <div class="media-body">
                        <h4><a href="https://users.cecs.anu.edu.au/~hongdong/" target="_blank">Hongdong Li</a></h4>
                        <p>Professor, Australian National University</p>
                        <p>Research: 3D vision reconstruction, structure from motion</p>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

	
<!-- Invited Speakers Section -->
	<h3>Invited Speakers</h3>
<style>
.info-row {
    display: flex;
    justify-content: space-between;
    margin-bottom: 10px;
}
.profile-card p {
    margin: 0;
}
.profile-card p:first-of-type {
    margin-top: 0;
}
.profile-card p:last-of-type {
    margin-bottom: 10px;
}
.profile-card h4 {
    margin-top: 0;
}
</style>

<!-- Torsten Sattler -->
<div class="profile-card">
    <div class="media">
        <div class="media-left">
            <img src="pic/speakers/Torsten.jpg" class="profile-img">
        </div>
        <div class="media-body">
            <h4><a href="https://tsattler.github.io/" target="_blank">Torsten Sattler</a></h4>
            <div class="info-row">
                <p><strong>Affiliation:</strong> Senior Researcher, Czech Technical University</p>
                <p><strong>Location:</strong> Europe</p>
            </div>
            <div class="info-row">
                <p><strong>Expertise:</strong> Robust 3D reconstruction and visual localization</p>
                <p><strong>Experience:</strong> Program Chair for DAGM GCPR'20, 3DV'22 General Chair</p>
            </div>
            <p><strong>Profile:</strong> Torsten Sattler is a Senior Researcher at CTU, where he heads the Spatial Intelligence group. His work is in the intersection of 3D computer vision and machine learning, with the goal of making 3D computer vision algorithms such as 3D reconstruction and visual localization more robust and reliable through scene understanding. Torsten has (co-)organized tutorials and workshops on visual localization at the main computer vision conference.</p>
        </div>
    </div>
</div>

<!-- Angela Dai -->
<div class="profile-card">
    <div class="media">
        <div class="media-left">
            <img src="pic/speakers/Dai.jpg" class="profile-img">
        </div>
        <div class="media-body">
            <h4><a href="https://www.professoren.tum.de/en/dai-angela" target="_blank">Angela Dai</a></h4>
            <div class="info-row">
                <p><strong>Affiliation:</strong> Associate Professor, Technical University of Munich</p>
                <p><strong>Location:</strong> Europe</p>
            </div>
            <div class="info-row">
                <p><strong>Expertise:</strong> 3D scene understanding and modeling</p>
                <p><strong>Awards:</strong> ERC Starting Grant, Eurographics Young Researcher Award</p>
            </div>
            <p><strong>Profile:</strong> Angela Dai is an Associate Professor at Technical University of Munich, where she leads the 3D AI group. Her research focuses on understanding how the 3D world can be modeled. Her research has been recognized through various prestigious awards, including the ERC Starting Grant, Eurographics Young Researcher Award, Google Research Scholar Award, and ZDB Junior Research Group Award.</p>
        </div>
    </div>
</div>

<!-- Noah Snavely -->
<div class="profile-card">
    <div class="media">
        <div class="media-left">
            <img src="pic/speakers/Noah.jpg" class="profile-img">
        </div>
        <div class="media-body">
            <h4><a href="https://www.cs.cornell.edu/~snavely/" target="_blank">Noah Snavely</a></h4>
            <div class="info-row">
                <p><strong>Affiliation:</strong> Professor, Cornell Tech & Google DeepMind</p>
                <p><strong>Location:</strong> The United States</p>
            </div>
            <div class="info-row">
                <p><strong>Expertise:</strong> 3D scene understanding from images</p>
                <p><strong>Awards:</strong> Sloan Fellowship, SIGGRAPH Significant New Researcher Award</p>
            </div>
            <p><strong>Profile:</strong> Noah Snavely is a Professor of Computer Science at Cornell Tech interested. He also works at Google DeepMind in NYC. His research interests are in computer vision and graphics, in particular in 3D understanding and depiction of scenes from images. Noah is the recipient of numerous prestigious awards, including a PECASE, a Microsoft New Faculty Fellowship, an Alfred P. Sloan Fellowship, and a SIGGRAPH Significant New Researcher Award.</p>
        </div>
    </div>
</div>

<!-- Nathan Jacobs -->
<div class="profile-card">
    <div class="media">
        <div class="media-left">
            <img src="pic/speakers/nathan.jpg" class="profile-img">
        </div>
        <div class="media-body">
            <h4><a href="https://jacobsn.github.io/" target="_blank">Nathan Jacobs</a></h4>
            <div class="info-row">
                <p><strong>Affiliation:</strong> Professor, Washington University in St. Louis</p>
                <p><strong>Location:</strong> The United States</p>
            </div>
            <div class="info-row">
                <p><strong>Expertise:</strong> Geospatial analysis from crowdsourced imagery</p>
                <p><strong>Funding:</strong> NSF, NIH, DARPA, IARPA, etc.</p>
            </div>
            <p><strong>Profile:</strong> Nathan Jacobs is a Professor at Washington University in St. Louis. His current focus is developing techniques for mining information about the natural world from geotagged imagery, including images from social networks, publicly available outdoor webcams, and satellites. His research has been funded by numerous prestigious organizations, including NSF, NIH, DARPA, IARPA, NGA, ARL, AFRL, and Google.</p>
        </div>
    </div>
</div>

<!-- Jingyi Yu -->
<div class="profile-card">
    <div class="media">
        <div class="media-left">
            <img src="pic/speakers/jingyi yu.jpg" class="profile-img">
        </div>
        <div class="media-body">
            <h4><a href="https://sist.shanghaitech.edu.cn/yujingyi_en/main.htm" target="_blank">Jingyi Yu</a></h4>
            <div class="info-row">
                <p><strong>Affiliation:</strong> Chair Professor, ShanghaiTech University</p>
                <p><strong>Location:</strong> Asia</p>
            </div>
            <div class="info-row">
                <p><strong>Expertise:</strong> Computational photography and camera design</p>
                <p><strong>Awards:</strong> NSF CAREER Award, AFOSR YIP Award</p>
            </div>
            <p><strong>Profile:</strong> Jingyi Yu is a Chair Professor and the Vice Provost of ShanghaiTech University. He also serves as the Dean of the School of Information Science and Technology at ShanghaiTech University. His research interests span a range of topics in computer vision and computer graphics, especially on computational photography and non-conventional optics and camera designs. He is a recipient of the NSF CAREER Award, the AFOSR YIP Award, and the Outstanding Junior Faculty Award at the University of Delaware.</p>
        </div>
    </div>
</div>

<!-- Diversity -->
	    <div class="card mb-4">
        <div class="card-header">
	<h3>Diversity</h3>

        </div>
        <div class="card-body">
            <p class="card-text"><strong>Organizers:</strong> (1) <strong>Gender balance:</strong> The team comprises three female and three male researchers, promoting gender inclusivity. (2) <strong>Different career stages:</strong> The committee includes two full professors, three assistant professors, and one postdoctoral researcher, representing a range of academic experiences and perspectives. (3) <strong>Geographical diversity:</strong> The organizers are affiliated with institutions across the globe, including Europe, America, Hong Kong, mainland China, and Australia, ensuring a broad representation of regional viewpoints and expertise.</p>
	    <p class="card-text">The confirmed speakers include (1) <strong>one female and four males</strong>, representing a wide range of research interests that align closely with the workshop’s theme. (2) <strong>Their expertise spans</strong> street level 3D reconstruction (e.g., urban scene understanding and ground-level visual localization), aerial vision systems (e.g., drone-based mapping and environmental monitoring), and satellite-based vision (e.g., large-scale geospatial analysis). This <strong>ensures comprehensive coverage of the workshop’s focus.</strong> (3) <strong>Geographically</strong>, the speakers are affiliated with leading institutions across Europe (e.g., Technical University of Munich, Czech Technical University in Prague), the United States (e.g., Cornell Tech, Washington University in St. Louis), and Asia (e.g., ShanghaiTech University), fostering a truly global exchange of ideas and collaboration.</p>
	</div>
    </div>

	
       <!-- Schedule Section -->
       <section id="format">
       <h2 class="section-title">4. Format and logistics</h2>
	       
        <h3>Workshop Schedule</h3>
    
        <div class="schedule-container">
            <!-- 早晨时段 -->
            <div class="time-slot morning">
                <div class="time-block">
                    <div class="time">08:30 - 08:35</div>
                    <div class="event">
                        <h4>Workshop Kickoff&Opening Comments</h4>
                    </div>
                </div>
                <div class="time-block">
                    <div class="time">08:35 - 09:05</div>
                    <div class="event">
                        <h4>Keynote Speech I</h4>
                    </div>
                </div>
                <div class="time-block">
                    <div class="time">09:05 - 09:35</div>
                    <div class="event">
                        <h4>Keynote Speech II</h4>
                    </div>
                </div>
                <div class="time-block">
                    <div class="time">09:35 - 10:05</div>
                    <div class="event">
                        <h4>Keynote Speech III</h4>
                    </div>
                </div>
            </div>

            <!-- 茶歇 -->
            <div class="time-slot break">
                <div class="time-block full-width">
                    <div class="time">10:05 - 11:00</div>
                    <div class="event">
                        <h4>Coffee Break & Poster Session</h4>
                    </div>
                </div>
            </div>

            <!-- 下午时段 -->
            <div class="time-slot afternoon">
                <div class="time-block">
                    <div class="time">11:00 - 11:30</div>
                    <div class="event">
                        <h4>Keynote Speech IV</h4>
                    </div>
                </div>
                <div class="time-block">
                    <div class="time">11:30 - 12:00</div>
                    <div class="event">
                        <h4>Keynote Speech V</h4>
                    </div>
                </div>
            </div>
        </div>

        <style>
            .schedule-container {
                max-width: 800px;
                margin: 0 auto;
                padding: 20px;
            }

            .time-slot {
                display: grid;
                grid-template-columns: repeat(2, 1fr);
                gap: 15px;
                margin-bottom: 15px;
            }

            .time-block {
                border: 1px solid #e0e0e0;
                border-radius: 8px;
                padding: 15px;
                background: white;
                transition: transform 0.2s;
            }

            .time-block:hover {
                transform: translateY(-3px);
                box-shadow: 0 3px 10px rgba(0,0,0,0.1);
            }

            .time {
                color: #3498db;
                font-weight: 600;
                margin-bottom: 8px;
                font-size: 0.9em;
            }

            .event h4 {
                color: #2c3e50;
                margin: 0 0 5px 0;
                font-size: 1.1em;
            }

            .event p {
                color: #7f8c8d;
                margin: 0;
                font-size: 0.9em;
            }

            /* 茶歇特殊样式 */
            .break .time-block {
                grid-column: 1 / -1;
                background: #f8f9fa;
                border-color: #3498db;
            }

            .break h4 {
                color: #3498db;
            }

            /* 响应式设计 */
            @media (max-width: 768px) {
                .time-slot {
                    grid-template-columns: 1fr;
                }
            
                .time-block {
                    padding: 12px;
                }
            
                .event h4 {
                    font-size: 1em;
                }
            }
        </style>
    </section>
    <!-- Paper submission -->
	    <div class="card mb-4">
        <div class="card-header">
	 <h3>Paper submission</h3>
        </div>
        <div class="card-body">
            <p class="card-text"><strong>Tentative program committee.</strong> We will invite the following experts as the potential program committee members: Zimin Xia (EPFL), Sijie Zhu(ByteDance), Scott Workman (DZYNE Technologies), Ted Lentsch (Delft University of Technology), Florian Fervers (Fraunhofer IOSB), Zhedong Zheng (National University of Singapore), Shruti Vyas (University of Central Florida), Brandon Clark (University of Central Florida), Aysim Toker (Technical University of Munich), Yanhao Zhang (University of Techonology Sydney), Dylan Campbell (Australian National University), Hongji Yang (Shenzhen University), Xiufan Lu (Shenzhen University), Gabriele Berton (Polytechnic of Turin), Krishna Regmi (University of Central Florida), Yingying Zhu (Shenzhen University), Shiming Wang (Delft University of Technology), Mubariz Zaffar (Delft University of Technology), et al.</p>
	    <p class="card-text"><strong>Paper review timeline.</strong> Paper submission: 5th July 2025; Author notification: 21th September 2025 Camera-ready: 21th October 2025 (TBD, will adhere to the main conference deadline)."</p>
	<p class="card-text"><strong>The accepted papers will be published in proceedings.</strong> Papers will adhere to the ICCV 2025 paper submission style, format, and length restrictions. The organizers promise to meet a deadline for providing the final documents to the publication chair to be included within the proceedings.</p>
    
	</div>
    </div>
    <!-- Competition -->
	    <div class="card mb-4">
        <div class="card-header">
		
            <h3>Competition</h3>
        </div>
        <div class="card-body">
            <p class="card-text">No, there is no competition in this workshop.</p>
	</div>
    </div>
	
	    <!-- Special requests -->
	    <div class="card mb-4">
        <div class="card-header">
            <h3>Special requests</h3>
        </div>
        <div class="card-body">
            <p class="card-text">No, there is no special request.</p>
	</div>
    </div>
	
<!-- Impacts Section -->
<section id="impacts">
  <h2 class="section-title">5. Impacts Section</h2>
 <div class="card mb-4">
	  <div class="card-header">
	</div>
        <div class="card-body">
            <p class="card-text">The broader impacts of this workshop lie in its potential to drive significant advancements in 3D scene understanding, modeling, and generation by integrating data from multiple altitudes, including ground, aerial, and satellite sources. This integration can enhance applications in urban planning, autonomous systems, and the development of digital twin cities. By fostering collaboration between these diverse data sources, the workshop will promote interdisciplinary research and inspire new directions for collaborative technologies that can improve the safety, efficiency, and sustainability of urban environments globally.</p>
                  </div>
    </div>
	
    <div class="card mb-4">
        <div class="card-header">
            <h3>Social Considerations</h3>
        </div>
        <div class="card-body">
            <p class="card-text">The exploration of multi-source data integration in this workshop raises social considerations about privacy and data security. The use of ground, aerial, and satellite data often involves personal and community-level information, and it is crucial to ensure equitable access to these technologies to prevent the deepening of social disparities. The workshop will highlight the need for inclusive solutions that promote fairness and community trust while ensuring that these technologies benefit all sections of society.</p> 
	</div>
    </div>
    
    <div class="card">
        <div class="card-header">
            <h3>Ethical Considerations</h3>
        </div>
        <div class="card-body">
            <p class="card-text">Developing technologies that rely on multi-source data fusion would raise concerns such as the misuse of data for surveillance purposes, where aerial and satellite imagery could be used to track individuals or monitor private spaces without consent. Moreover, ensuring that the algorithms and models used are free from biases is critical to avoid discriminatory outcomes. The workshop will explore how to design these systems with a focus on transparency, accountability, and fairness, ensuring that technological advancements are ethically sound and serve the greater good.</p>
	</div>
    </div>
</section>

<!-- Related Workshops Section -->
<section id="related">
    <h2 class="section-title">6. Related Workshops</h2>
    <div class="card mb-4">
    <div class="card">
        <div class="card-header">
        </div>
        <div class="card-body">
            <p class="card-text">This workshop aims to address key challenges in 3D scene understanding, modeling, and data fusion across multiple altitudes, integrating both ground-level and aerial perspectives. While it shares similarities with workshops such as EarthVision: Large Scale Computer Vision for Remote Sensing Imagery (CVPR 2021-2024) and Scalable 3D Scene Generation and Geometric Scene Understanding (ECCV 2024), our workshop offers a unique focus.</p>
	    <p class="card-text"><strong>Unlike EarthVision</strong>, which primarily concentrates on satellite imagery for large-scale environmental monitoring, our workshop explores the collaboration across multiple altitude data sources—including ground, drone-based aerial, and satellite images—to enhance scene modeling and understanding.</p>
	    <p class="card-text"><strong>In comparison to the ECCV 2024 workshop on Scalable 3D Scene Generation</strong>, which focuses on large-scale 3D scene generation, this workshop emphasizes how drone-based aerial and satellite data can complement ground-level data to achieve a more complete and accurate understanding of the world and its 3D modeling.
	</div>
    </div>
</section>


<footer class="text-center mt-5 py-4 border-top">
    <p class="mb-1">© 2025 3D-VAST Workshop Committee</p>
    <p>Contact: <a href="mailto:shiyj2@shanghaitech.edu.cn" class="text-reset text-decoration-none">shiyj2@shanghaitech.edu.cn</a></p>
</footer>

<!-- Scripts -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.0/dist/js/bootstrap.bundle.min.js"></script>
