 

<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  <meta name="keywords" content="workshop, computer vision, audio processing, computer graphics, visual learning, machine learning">

  <link rel="shortcut icon" href="static/img/site/favicon.png">

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
  <link rel="stylesheet" href="static/css/main.css" media="screen,projection">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

  <style>

    .people-pic {
      max-width: 175px;
      max-height: 175px;
      /*width:300px;*/
      /*height:300px;*/
      /*object-fit: cover;*/
      object-fit: scale-down;
      border-radius: 50%;
  }
  </style>
</head>

  <body>

    <!-- <div class="top-strip"></div> -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="container">
    
    <div class="navbar-header">
      <a class="navbar-brand" href="/"></a>
      <button class="navbar-toggle" type="button" data-toggle="collapse" data-target="#navbar-main">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>

    <div class="navbar-collapse collapse" id="navbar-main">
      <ul class="nav navbar-nav">
        <li><a href="#intro">Introduction</a></li>
        <li><a href="#cfp">Call for papers</a></li>
        <li><a href="#dates">Schedule</a></li>
        <li><a href="#wsp">Poster Presentation</a></li>  
        <!--<li><a href="#wsp">Poster Presentation</a></li> -->     
        <li><a href="#speakers">Invited Speakers</a></li>
        <li><a href="#organizers">Organizers</a></li>
        <li><a href="#contact">Contact</a></li>
        <!-- <li class="dropdown">
          <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Past Workshops <span class="caret"></span></a>
          <ul class="dropdown-menu">
            <li><a href="../CVPR2021/index.html" target="__blank">CVPR 2021</a></li>
            <li><a href="../ICCV2022/" target="__blank">ICCV 2022</a></li>
          </ul>
        </li> -->
      </ul>
    </div>

  </div>
</div>


    <div class="container">
      <div class="page-content">
          <p><br /></p>
<div class="row">
  <div class="col-xs-12">
    <center><h1>1st Workshop on 3D-VAST</h1></center>
    <center><h1>From street to space: 3D Vision AcrosS alTitudes</h1></center>
    <center><h2>ICCV 2025 Workshop</h2></center>
<!--     <center><strong>Room Amber 6</strong> - September 29th (9:00am - 1:00pm), 2025(TBD)</center> -->
  </div>
</div>

<hr />

<p><br /></p>
<div class="row" id="intro">
  <div class="col-xs-12">
    <h2>Introduction</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
        As large-scale 3D scene modeling becomes increasingly important for applications such as urban planning, robotics, autonomous navigation, and virtual simulations, the need for diverse, high-quality
visual data is greater than ever. However, acquiring dense and high-resolution ground-level imagery
at scale is often impractical due to access limitations, cost, and environmental variability. In contrast,
aerial and satellite imagery provide broader spatial coverage but lack the fine-grained details needed for
many downstream applications. Combining images from multiple altitudes — from ground cameras to
aerial drones and satellites—offers a promising solution to overcome these limitations, enabling richer,
more complete 3D reconstructions.   </p>
    <p>
      How can we achieve coherent and accurate 3D scene modeling <strong>when our visual world is captured from vastly different altitudes—ground, aerial, and satellite—under varying conditions?</strong> Each altitude offers distinct advantages, but cross-altitude data fusion introduces significant
challenges: sparse and incomplete views, visual ambiguities, spatio-temporal inconsistencies, image
quality variations, dynamic scene changes, and environmental factors that alter topology over time.
Traditional 3D reconstruction methods, optimized for dense and structured inputs, struggle with such
heterogeneous multi-altitude data. Advances in multi-scale feature alignment, neural scene representations, and robust cross-view fusion offer promising solutions, but key challenges remain.
      </p>
  </div>
</div>

<p><br /></p>
<div class="row" id="cfp">
  <div class="col-xs-12">
    
    <h2>Call For Papers</h2>
</div>
</div>
<div class="row">
  <div class="col-xs-12">
      <p>
        <span style="font-weight:500;">Call for papers:</span> We invite non-archival papers of up to 8 pages (in ICCV format) for work 
        on tasks related to cross-altitude 3D scene modeling, understanding, rendering, and synthesis. 
        Paper topics may include, but are not limited to:
      </p>
      <ul>
        <li>Cross-altitude feature matching and registration</li>
        <li>View synthesis from sparse and heterogeneous data sources</li>
        <li>Sparse-view 3D reconstruction (with known or unknown camera poses)</li>
        <li>Generative approaches for view completion and prediction</li>
        <li>Datasets and benchmarks for evaluating cross-altitude vision systems</li>
        <li>Real-world applications in urban planning, simulation, and digital twins</li>
      </ul>
      <p>
      <p>
        While the workshop focuses on 3D vision across different altitudes, the paper topic could center on a specific data source, such as outdoor ground-level, aerial-level, or indoor environments. 
      <p>

        <span style="font-weight:500;">Submission:</span> We encourage submissions of up to 8 pages, excluding references and acknowledgements.
        The submission should be in the ICCV format.
        Reviewing will be double-blind.
        Please submit your paper to the following address by the deadline: 
         <a href="https://openreview.net/group?id=thecvf.com/ICCV/2025/Workshop/3D-VAST">Submission Portal</a>
      </p>
  </div>
</div>
        
<p><br /></p>
<div class="row" id="wsp">
  <div class="col-xs-12">
<p><br /></p>
   
<p><br /></p>
<div class="row" id="wsp">
  <div class="col-xs-12">
    <h2>Poster Presentation</h2>
  </div>
</div>
<div class="row">
  <div class="col-md-12">
    <table>
      <tbody> 
      <tr><td><a href=" ">#1. Adapting Stereo Vision From Objects To 3D Lunar Surface Reconstruction with the StereoLunar Dataset</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Clémentine Grethen, Simone Gasparini, Géraldine Morin, Jérémy Lebreton, Lucas Marti, Manuel Sanchez-Gestido </font></td></tr>
      <tr><td><a href=" ">#2. MIRAGE: Unsupervised Single Image to Novel View Generation with Cross Attention Guidance</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Llukman Cerkezi, Aram Davtyan, Sepehr Sameni, Paolo Favaro </font></td></tr>
      <tr><td><a href=" ">#3. 3DGS-to-PC: 3D Gaussian Splatting to a Dense Point Cloud</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Lewis A G Stuart, Andrew Morton, Ian Stavness, Michael P. Pound </font></td> </tr>        
      <tr><td><a href=" ">#4. Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation</a> <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Camille Billouard, Dawa Derksen, Alexandre Constantin, Bruno Vallet </font></td></tr>
      <tr><td><a href=" ">#5. Fusing Convolution and Vision Transformer Encoders for Object Height Estimation from Monocular Satellite and Aerial Images</a> <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Furkan Gültekin, Alper Koz, Reza Bahmanyar, Seyedmajid Azimi, Mehmet Lutfi Suzen </font></td></tr>
      <tr><td><a href=" ">#6. GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Shaowen Tong, Zimin Xia, Alexandre Alahi, Xuming He, Yujiao Shi </font></td> </tr>        
      <tr><td><a href=" ">#7. Where am I? Cross-View Geo-localization with Natural Language Descriptions</a> <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Junyan Ye, Honglin Lin, Leyan Ou, Dairong Chen, Zihao Wang, Qi Zhu, Conghui He, Weijia Li </font></td></tr>
      <tr><td><a href=" ">#8. Skydiffusion: Street-to-satellite image synthesis with diffusion models and BEV paradigm</a> <br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<font color="gray">Junyan Ye, Jun He, Weijia Li, Zhutao Lv, Yi Lin, Jinhua Yu, Haote Yang, Conghui He </font></td></tr>
      </tbody>
      </table>
  </div>
</div>
      
        
<p><br /></p>

   

 
<div class="row" id="dates">
  <div class="col-xs-12">
    <h2>Important Dates</h2>
  </div>
</div>

<div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
       <tr>
        There is no difference between the two rounds. Papers accepted by both rounds will be published in the ICCV proceedings.
       </tr>
        <!-- First Round Section -->
        <tr>
          <td colspan="2"><strong>First Round</strong></td>
        </tr>
        <tr>
          <td>Paper submission deadline</td>
          <td>June 5th, 2025</td>
        </tr>
        <tr>
          <td>Notifications to accepted papers</td>
          <td>June 24th, 2025</td>
        </tr>
        <tr>
          <td>Paper camera ready</td>
          <td>August 1st, 2025 @ 11:59 Pacific Time.</td>
        </tr>
        
        <!-- Second Round Section -->
        <tr>
          <td colspan="2"><strong>Second Round</strong></td>
        </tr>
        <tr>
          <td>Paper submission deadline</td>
          <td style="color: red;">July 5th, 2025</td>
        </tr>
        <tr>
          <td>Notifications to accepted papers</td>
          <td style="color: red;">July 10th, 2025</td>
        </tr>
        <tr>
          <td>Paper camera ready</td>
          <td>August 1st, 2025 @ 11:59 Pacific Time.</td>
        </tr>
        <tr>
          <td>Workshop date</td>
          <td>Oct 20th, 2025</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>
    
<!-- <div class="row">
  <div class="col-xs-12">
    <table class="table table-striped">
      <tbody>
        <tr>
         <b>First Round</b> (for papers which aim to be published in the ICCV proceedings)<b>:</b>
        </tr>
        <tr>
          <td>Paper submission deadline</td>
          <td>June 5th, 2025</td>
        </tr>
        <tr>
          <td>Notifications to accepted papers</td>
          <td>June 24th, 2025</td>
        </tr>
        <tr>
          <td>Paper camera ready</td>
          <td>TBD</td>
        </tr>
       <tr>
         <b>Second Round</b> (for papers which do not aim to be included in the ICCV proceedings, but will be presented in the workshop)<b>:</b>
        </tr>
        <tr>
          <td>Paper submission deadline</td>
          <td>July 5th, 2025</td>
        </tr>
        <tr>
          <td>Notifications to accepted papers</td>
          <td>July 24th, 2025</td>
        </tr>
        <tr>
          <td>Paper camera ready</td>
          <td>TBD</td>
        </tr>
        <tr>
          <td>Workshop date</td>
          <td>TBD </td>
        </tr>
      </tbody>
    </table>
  </div>
</div> -->

<p><br /></p>
<div class="row" id="schedule">
  <div class="col-xs-12">
    <h2>Schedule</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
     <table class="table table-striped">
      <tbody>
        <tr>
          <td>Workshop Kickoff and Opening Comments</td>
          <td>8:30am - 8:35am</td>
        </tr>
        <tr>
          <td>Angela Dai<br/> <em>Title: Structured Priors for Generating 3D.</em></td>
          <td>8:35am - 9:05am</td>
        </tr>
         <tr>
          <td>Noah Snavely<br/> <em>Title: Abstract Plan Views and Photos</em></td>
          <td>9:05am - 9:35sm</td>
        </tr>
         <tr>
          <td>Jingyi Yu<br/> <em>Title: Scene Generation Across Scales</em></td>
          <td>9:35am - 10:05am</td>
        </tr>
          <tr>
          <td>Coffee Break and Poster Session</td>
          <td>10:05am - 11:00am</td>
        </tr>
        <tr>
          <td>Torsten Sattler<br/><em>Title: A Personal and Biased Guide to Choosing Scene Representations for Visual Localization
</em> </td>
          <td>11:00am - 11:30am</td>
        </tr>
         <tr>
          <td>Nathan Jacobs<br/><em>Title: TBD</em> </td>
          <td>11:30am - 12:00am</td>
        </tr>
      </tbody>
    </table>
  </div>
</div>

<p><br /></p>
<div class="row" id="speakers">
  <div class="col-xs-12">
    <h2>Invited Speakers</h2>
  </div>
</div>

<style>
  .people-pic {
    width: 130px; /* 设置统一的宽度 */
    height: 130px; /* 设置统一的高度 */
    object-fit: cover; /* 保持图片的宽高比，同时覆盖整个区域 */
  }
</style>

<p><br /></p>
<!-- Torsten Sattler -->
<div class="row">
  <div class="col-md-2">
    <a href="https://tsattler.github.io/">
      <img class="people-pic" src="pic/speakers/Torsten.jpg" />
    </a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://tsattler.github.io/">Torsten Sattler</a></b> is a Senior Researcher at CTU, where he heads the Spatial
      Intelligence group. His work is in the intersection of 3D computer vision and machine learning, with
      the goal of making 3D computer vision algorithms such as 3D reconstruction and visual localization
      more robust and reliable through scene understanding. Torsten has (co-)organized tutorials and workshops on visual localization at the main computer vision conference, was a program chair for DAGM
      GCPR’20, a general chair for 3DV’22, and a program chair for ICCV’24.
    </p>
  </div>
</div>
<p><br /></p>

<!-- Angela Dai -->
<div class="row">
  <div class="col-md-2">
    <a href="https://www.professoren.tum.de/en/dai-angela">
      <img class="people-pic" src="pic/speakers/Dai.jpg" />
    </a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://www.professoren.tum.de/en/dai-angela">Angela Dai</a></b> is an Associate Professor at Technical University of Munich,
      where she leads the 3D AI group. Her research focuses on understanding how the 3D world can be
      modeled. Her research has been recognized through an ERC Starting Grant, Eurographics Young
      Researcher Award, Google Research Scholar Award, ZDB Junior Research Group Award, an ACM
      SIGGRAPH Outstanding Doctoral Dissertation Honorable Mention, as well as a Stanford Graduate
      Fellowship.
       </p>
    <p>Title: Structured Priors for Generating 3D. </p>
  </div>
</div>
<p><br /></p>

<!-- Noah Snavely -->
<div class="row">
  <div class="col-md-2">
    <a href="https://www.cs.cornell.edu/~snavely/">
      <img class="people-pic" src="pic/speakers/Noah.jpg" />
    </a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://www.cs.cornell.edu/~snavely/">Noah Snavely</a></b> is a Professor of Computer Science at Cornell
      Tech interested. He also works at Google DeepMind in NYC. His research interests are in computer
      vision and graphics, in particular in 3D understanding and depiction of scenes from images. Noah is
      the recipient of a PECASE, a Microsoft New Faculty Fellowship, an Alfred P. Sloan Fellowship, and a
      SIGGRAPH Significant New Researcher Award, and is a Fellow of the ACM and the IEEE.
    </p>
  </div>
</div>
<p><br /></p>

<!-- Nathan Jacobs -->
<div class="row">
  <div class="col-md-2">
    <a href="https://jacobsn.github.io/">
      <img class="people-pic" src="pic/speakers/nathan.jpg" />
    </a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://jacobsn.github.io/">Nathan Jacobs</a></b> is a Professor at Washington University in
      St. Louis. His current focus is developing techniques for mining information about the natural world
      from geotagged imagery, including images from social networks, publicly available outdoor webcams,
      and satellites. His research has been funded by NSF, NIH, DARPA, IARPA, NGA, ARL, AFRL, and
      Google.
    </p>
  </div>
</div>

<!-- Jingyi Yu -->
<p><br /></p>
<div class="row">
  <div class="col-md-2">
    <a href="https://sist.shanghaitech.edu.cn/yujingyi_en/main.htm">
      <img class="people-pic" src="pic/speakers/jingyi yu.jpg" />
    </a>
  </div>
  <div class="col-md-10">
    <p>
      <b><a href="https://sist.shanghaitech.edu.cn/yujingyi_en/main.htm">Jingyi Yu</a></b> is a Chair Professor and the Vice Provost of ShanghaiTech
      University. He also serves as the Dean of the School of Information Science and Technology at ShanghaiTech University. His research interests span a range of topics in computer vision and computer
      graphics, especially on computational photography and non-conventional optics and camera designs.
      He is a recipient of the NSF CAREER Award, the AFOSR YIP Award, and the Outstanding Junior
      Faculty Award at the University of Delaware.
        </p>
    <p>Title: Mid-level Vision. </p>
  </div>
</div>

        
<p><br /></p>
<div class="row" id="organizers">
  <div class="col-xs-12">
    <h2>Organizers</h2>
  </div>
</div>

<div class="row">
    <div class="col-xs-2">
        <a href="https://yujiaoshi.github.io/">
          <img class="people-pic" src="pic/Organizers/Yujiao Shi.jpg" />
        </a>
        <div class="people-name">
          <a href="https://yujiaoshi.github.io/">Yujiao Shi</a>
          <h6>Assistant Professor, ShanghaiTech University</h6>
        </div>
      </div>

  <div class="col-xs-2">
    <a href="https://kam1107.github.io/">
        <img class="people-pic" src="pic/Organizers/ambie.jpg" />      
    </a>
    <div class="people-name">
      <a href="https://kam1107.github.io/">Yuanbo Xiangli</a>
      <h6>Postdoctoral Researcher, Cornell University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://cmp.felk.cvut.cz/~kukelova/">
      <img class="people-pic" src="pic/Organizers/Zuzana Kukelova.jpg" />
    </a>
    <div class="people-name">
      <a href="https://cmp.felk.cvut.cz/~kukelova/">Zuzana Kukelova</a>
      <h6>Assistant Professor, Czech Technical University</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://datascience.hku.hk/people/bo-dai/">
      <img class="people-pic" src="pic/Organizers/Bo Dai.jpg" />
    </a>
    <div class="people-name">
      <a href="https://datascience.hku.hk/people/bo-dai/">Bo Dai</a>
      <h6>Assistant Professor, The University of Hong Kong</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://comp.anu.edu.au/people/richard-hartley/">
      <img class="people-pic" src="pic/Organizers/Richard-Hartley.png" />
    </a>
    <div class="people-name">
      <a href="https://comp.anu.edu.au/people/richard-hartley/">Richard Hartley</a>
      <h6>Distinguished Professor Emeritus, ANU</h6>
    </div>
  </div>

  <div class="col-xs-2">
    <a href="https://users.cecs.anu.edu.au/~hongdong/">
      <img class="people-pic" src="pic/Organizers/Hongdong LI.jpg" />
    </a>
    <div class="people-name">
      <a href="https://users.cecs.anu.edu.au/~hongdong/">Hongdong Li</a>
      <h6>Professor, Australian National University</h6>
    </div>
  </div>

</div>
<p><br /></p>
<p><br /></p>

<div class="row" id="contact">
  <div class="col-xs-12">
    <h2>Contact</h2>
  </div>
</div>
<div class="row">
  <div class="col-xs-12">
    <p>
      To contact the organizers please use <b>shiyj2@shanghaitech.edu.cn,com</b>
    </p>
  </div>
</div>
<p><br /></p>

<hr />

<div class="row">
  <div class="col-xs-12">
    <h2>Acknowledgments</h2>
  </div>
</div>
<p><a name="/acknowledgements"></a></p>
<div class="row">
  <div class="col-xs-12">
    <p>
      Thanks to <span style="color:#1a1aff;font-weight:400;"> <a href="https://visualdialog.org/">visualdialog.org</a></span> for the webpage format.
    </p>
  </div>
</div>

      </div>
    </div>

  </body>
</html>
